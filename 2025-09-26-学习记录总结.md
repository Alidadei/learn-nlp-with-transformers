# Transformerå®Œæ•´å­¦ä¹ è®°å½•ï¼ˆ2025-09-26ï¼‰

## ç›®å½•
- [ç¯å¢ƒé…ç½®](#ç¯å¢ƒé…ç½®)
- [PythonåŸºç¡€æ¦‚å¿µ](#pythonåŸºç¡€æ¦‚å¿µ)
- [Transformeræ ¸å¿ƒåŸç†](#transformeræ ¸å¿ƒåŸç†)
- [æ³¨æ„åŠ›æœºåˆ¶æ·±å…¥ç†è§£](#æ³¨æ„åŠ›æœºåˆ¶æ·±å…¥ç†è§£)
- [å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶](#å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶)
- [å¹¶è¡Œè®¡ç®—ä¸çŸ©é˜µè¿ç®—](#å¹¶è¡Œè®¡ç®—ä¸çŸ©é˜µè¿ç®—)
- [QKVæœºåˆ¶è¯¦è§£](#qkvæœºåˆ¶è¯¦è§£)
- [Dropoutæœºåˆ¶æ·±å…¥ç†è§£](#dropoutæœºåˆ¶æ·±å…¥ç†è§£)
- [å®é™…åº”ç”¨ä¸ä»£ç å®ç°](#å®é™…åº”ç”¨ä¸ä»£ç å®ç°)
- [æ€»ç»“ä¸å…³é”®è¦ç‚¹](#æ€»ç»“ä¸å…³é”®è¦ç‚¹)

---

## ç¯å¢ƒé…ç½®

### æœ¬åœ°ç¯å¢ƒé…ç½®æµç¨‹
1. **ä¸‹è½½é¡¹ç›®**ï¼šé€šè¿‡GitHubä¸‹è½½ZIPæˆ–ä½¿ç”¨git cloneå‘½ä»¤
2. **æ‰“å¼€é¡¹ç›®**ï¼šç”¨VS Codeæ‰“å¼€é¡¹ç›®æ–‡ä»¶å¤¹
3. **åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ**ï¼š`python -m venv venv`
4. **æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ**ï¼š
   ```bash
   # Windows PowerShell
   .\venv\Scripts\Activate.ps1
   # å¯èƒ½éœ€è¦å…ˆè®¾ç½®æ‰§è¡Œç­–ç•¥
   Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
   ```
5. **å‡çº§pip**ï¼š`python -m pip install --upgrade pip`
6. **å®‰è£…ä¾èµ–**ï¼š`pip install -r requirements.txt`
7. **é€‰æ‹©è§£é‡Šå™¨**ï¼šåœ¨VS Codeä¸­é€‰æ‹©è™šæ‹Ÿç¯å¢ƒä½œä¸ºPythonè§£é‡Šå™¨

### Kaggleç¯å¢ƒé…ç½®
```python
# 1. å…‹éš†é¡¹ç›®
!git clone https://github.com/datawhalechina/learn-nlp-with-transformers.git
%cd learn-nlp-with-transformers

# 2. å®‰è£…å…¼å®¹ä¾èµ–ï¼ˆè·³è¿‡è¿‡æ—¶çš„requirements.txtï¼‰
!pip install transformers>=4.20.0 --quiet
!pip install datasets>=2.0.0 --quiet
!pip install torch>=1.12.0 --quiet
!pip install tokenizers>=0.12.0 --quiet

# 3. éªŒè¯å®‰è£…
import transformers, datasets, torch
print(f"âœ… Transformers: {transformers.__version__}")
print(f"âœ… Datasets: {datasets.__version__}")
print(f"âœ… PyTorch: {torch.__version__}")
print(f"âœ… GPUå¯ç”¨: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"âœ… GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}")
```

### ç¯å¢ƒé…ç½®å…³é”®æ¦‚å¿µ
- **è™šæ‹Ÿç¯å¢ƒä½œç”¨**ï¼šé¡¹ç›®ä¾èµ–éš”ç¦»ï¼Œé¿å…ç‰ˆæœ¬å†²çªï¼Œä¿æŠ¤ç³»ç»Ÿç¯å¢ƒ
- **ä¸ºä»€ä¹ˆæ¯ä¸ªé¡¹ç›®éœ€è¦è™šæ‹Ÿç¯å¢ƒ**ï¼šä¸åŒé¡¹ç›®å¯èƒ½éœ€è¦ä¸åŒç‰ˆæœ¬çš„åŒ…ï¼Œè™šæ‹Ÿç¯å¢ƒç¡®ä¿ç¯å¢ƒä¸€è‡´æ€§
- **batch_sizeâ‰ æ³¨æ„åŠ›å¤´æ•°é‡**ï¼šbatch_sizeæ˜¯åŒæ—¶å¤„ç†çš„å¥å­æ•°é‡ï¼Œæ³¨æ„åŠ›å¤´æ˜¯æ¨¡å‹å†…éƒ¨çš„è®¡ç®—å•å…ƒ
- **å·¥ç¨‹æ–‡ä»¶å­˜æ”¾ä½ç½®**ï¼šé¡¹ç›®æ–‡ä»¶ä¸venvç›®å½•åŒçº§ï¼Œä¸æ”¾åœ¨venvå†…éƒ¨

---

## PythonåŸºç¡€æ¦‚å¿µ

### é‡è¦è¿ç®—ç¬¦å’Œè¯­æ³•

#### `!` ç¬¦å·çš„ä½¿ç”¨
- **Jupyter Notebook/Colab/Kaggleä¸­**ï¼š`!pip install transformers` - æ‰§è¡Œç³»ç»Ÿå‘½ä»¤
- **ç»ˆç«¯/å‘½ä»¤è¡Œä¸­**ï¼šç›´æ¥ä½¿ç”¨ `pip install transformers` - ä¸éœ€è¦!ç¬¦å·
- **åŒºåˆ«åŸå› **ï¼šNotebookç¯å¢ƒéœ€è¦åŒºåˆ†Pythonä»£ç å’Œç³»ç»Ÿå‘½ä»¤

#### `//` æ•´æ•°é™¤æ³•è¿ç®—ç¬¦
```python
# æ™®é€šé™¤æ³•
300 / 6 = 50.0    # ç»“æœæ˜¯æµ®ç‚¹æ•°

# æ•´æ•°é™¤æ³•  
300 // 6 = 50     # ç»“æœæ˜¯æ•´æ•°

# ä¸ºä»€ä¹ˆåœ¨Transformerä¸­ä½¿ç”¨//ï¼Ÿ
head_dim = hid_dim // n_heads  # ç¡®ä¿æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦æ˜¯æ•´æ•°
assert hid_dim % n_heads == 0  # ç¡®ä¿èƒ½å¤Ÿæ•´é™¤
```

#### `super().__init__()`çš„ä½œç”¨
```python
class MultiheadAttention(nn.Module):
    def __init__(self, hid_dim, n_heads, dropout):
        super(MultiheadAttention, self).__init__()  # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        # å­ç±»ç‰¹æœ‰çš„åˆå§‹åŒ–ä»£ç 
```

**ä½œç”¨**ï¼š
- ç»§æ‰¿PyTorchæ¨¡å—çš„æ ¸å¿ƒåŠŸèƒ½ï¼ˆå‚æ•°ç®¡ç†ã€GPUè½¬æ¢ã€è®­ç»ƒæ¨¡å¼åˆ‡æ¢ï¼‰
- ç¡®ä¿æ¨¡å—æ­£å¸¸å·¥ä½œï¼ˆå‚æ•°æ³¨å†Œã€æ¢¯åº¦è®¡ç®—ã€æ¨¡å‹ä¿å­˜ï¼‰
- **å¿…é¡»åœ¨å­ç±»æ„é€ å‡½æ•°å¼€å¤´è°ƒç”¨**

---

## Transformeræ ¸å¿ƒåŸç†

### å®è§‚æ¶æ„
- **ç¼–ç å™¨ï¼ˆEncoderï¼‰**ï¼š6å±‚ï¼Œæ¯å±‚åŒ…å«Self-Attention + FFN
- **è§£ç å™¨ï¼ˆDecoderï¼‰**ï¼š6å±‚ï¼Œæ¯å±‚åŒ…å«Self-Attention + Encoder-Decoder Attention + FFN
- **ä¼˜åŠ¿**ï¼šç›¸æ¯”RNNï¼Œå¯ä»¥å¹¶è¡Œå¤„ç†æ•´ä¸ªåºåˆ—ï¼Œä¸éœ€è¦æŒ‰æ—¶é—´æ­¥é€’å½’

### è¾“å…¥å¤„ç†

#### è¯å‘é‡ï¼ˆWord Embeddingsï¼‰
```python
# è¯æ±‡è¡¨ç¤ºä¾‹
word_embeddings = {
    "Thinking": [0.2, -0.1, 0.5, 0.3, ..., 0.8],  # 300ä¸ªæ•°å­—
    "Machines": [-0.3, 0.7, -0.2, 0.1, ..., 0.4], # 300ä¸ªæ•°å­—
}
# æ¯ä¸ªè¯ç”¨300ç»´å‘é‡è¡¨ç¤ºå…¶è¯­ä¹‰ç‰¹å¾
```

#### ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰
```python
# ä½ç½®ç¼–ç å…¬å¼
PE(pos,2i) = sin(pos / 10000^(2i/d_model))
PE(pos,2i+1) = cos(pos / 10000^(2i/d_model))
```
- **ä½œç”¨**ï¼šä¸ºæ¨¡å‹æä¾›è¯æ±‡ä½ç½®ä¿¡æ¯å’Œè¯é—´è·ç¦»ç‰¹å¾
- **ä¼˜åŠ¿**ï¼šå¯æ‰©å±•åˆ°æœªçŸ¥åºåˆ—é•¿åº¦

### æ®‹å·®è¿æ¥ä¸å±‚æ ‡å‡†åŒ–
```python
# æ¯ä¸ªå­å±‚éƒ½æœ‰æ®‹å·®è¿æ¥
output = LayerNorm(x + SubLayer(x))
```
- **æ®‹å·®è¿æ¥**ï¼šå¸®åŠ©æ¢¯åº¦ä¼ æ’­ï¼Œé¿å…æ¢¯åº¦æ¶ˆå¤±
- **å±‚æ ‡å‡†åŒ–**ï¼šç¨³å®šè®­ç»ƒè¿‡ç¨‹

---

## æ³¨æ„åŠ›æœºåˆ¶æ·±å…¥ç†è§£

### Self-Attentionè®¡ç®—æ­¥éª¤

#### å•è¯çº§åˆ«ç†è§£
ä»¥å¥å­"Thinking Machines"ä¸ºä¾‹ï¼š
```python
# 1. è®¡ç®—Qã€Kã€V
q1 = X1 @ W_Q  # "Thinking"çš„Query
q2 = X2 @ W_Q  # "Machines"çš„Query
k1 = X1 @ W_K  # "Thinking"çš„Key  
k2 = X2 @ W_K  # "Machines"çš„Key
v1 = X1 @ W_V  # "Thinking"çš„Value
v2 = X2 @ W_V  # "Machines"çš„Value

# 2. è®¡ç®—attentionåˆ†æ•°
score_11 = q1 Â· k1 / âˆšd_k  # Thinkingå¯¹Thinking
score_12 = q1 Â· k2 / âˆšd_k  # Thinkingå¯¹Machines
score_21 = q2 Â· k1 / âˆšd_k  # Machineså¯¹Thinking  
score_22 = q2 Â· k2 / âˆšd_k  # Machineså¯¹Machines

# 3. Softmaxå½’ä¸€åŒ–
attention_weights = softmax([score_11, score_12])

# 4. åŠ æƒæ±‚å’ŒValue
z1 = v1 Ã— attention_11 + v2 Ã— attention_12
```

#### çŸ©é˜µè®¡ç®—å½¢å¼
```python
# çŸ©é˜µå½¢å¼çš„Self-Attention
Q = X @ W_Q  # [seq_len, d_model] @ [d_model, d_k] = [seq_len, d_k]
K = X @ W_K  # [seq_len, d_model] @ [d_model, d_k] = [seq_len, d_k]
V = X @ W_V  # [seq_len, d_model] @ [d_model, d_v] = [seq_len, d_v]

# è®¡ç®—attention
Attention(Q,K,V) = softmax(QK^T / âˆšd_k)V
```

### Maskæœºåˆ¶
```python
# Padding Mask - å±è”½å¡«å……ä½ç½®
mask = [
    [1, 1, 1, 0, 0],  # å‰3ä¸ªè¯æœ‰æ•ˆï¼Œå2ä¸ªæ˜¯padding
    [1, 1, 1, 1, 0],  # å‰4ä¸ªè¯æœ‰æ•ˆï¼Œæœ€å1ä¸ªæ˜¯padding
]

# Future Mask - è§£ç å™¨ä¸­å±è”½æœªæ¥ä½ç½®
future_mask = [
    [1, 0, 0, 0],  # ç¬¬1ä¸ªè¯åªèƒ½çœ‹è‡ªå·±
    [1, 1, 0, 0],  # ç¬¬2ä¸ªè¯èƒ½çœ‹å‰2ä¸ª
    [1, 1, 1, 0],  # ç¬¬3ä¸ªè¯èƒ½çœ‹å‰3ä¸ª
    [1, 1, 1, 1]   # ç¬¬4ä¸ªè¯èƒ½çœ‹å‰4ä¸ª
]

# åº”ç”¨maskï¼šå°†mask=0çš„ä½ç½®è®¾ä¸º-1e10
if mask is not None:
    attention = attention.masked_fill(mask == 0, -1e10)
# ç»è¿‡softmaxåï¼Œ-1e10çš„ä½ç½®æƒé‡â‰ˆ0
```

---

## å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶

### ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›å¤´
**æ³¨æ„åŠ›å¤´**æ˜¯å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„ç‹¬ç«‹æ³¨æ„åŠ›è®¡ç®—å•å…ƒï¼Œæ¯ä¸ªå¤´éƒ½æœ‰è‡ªå·±çš„å‚æ•°çŸ©é˜µï¼Œå­¦ä¹ ä¸åŒç±»å‹çš„è¯­è¨€å…³ç³»ã€‚

### å¤šå¤´æ³¨æ„åŠ›çš„å¿…è¦æ€§
1. **æ‰©å±•æ¨¡å‹å…³æ³¨ä¸åŒä½ç½®çš„èƒ½åŠ›**
2. **æä¾›å¤šä¸ªå­è¡¨ç¤ºç©ºé—´**ï¼šæ¯ä¸ªå¤´å­¦ä¹ ä¸åŒçš„è¯­è¨€æ¨¡å¼

#### å®é™…ä¾‹å­
å¥å­ï¼š"The animal didn't cross the street because it was too tired"
- ğŸŸ  **æ©™è‰²æ³¨æ„åŠ›å¤´**ï¼šå­¦ä¹ æŒ‡ä»£å…³ç³»ï¼ˆ"it" â†’ "The animal"ï¼‰
- ğŸŸ¢ **ç»¿è‰²æ³¨æ„åŠ›å¤´**ï¼šå­¦ä¹ å› æœå…³ç³»ï¼ˆ"it" â†’ "tired"ï¼‰
- **å…¶ä»–å¤´**ï¼šè¯­æ³•å…³ç³»ã€ä½ç½®å…³ç³»ã€è¯­ä¹‰ç›¸ä¼¼æ€§ç­‰

### è®¡ç®—è¿‡ç¨‹è¯¦è§£

#### å‚æ•°çŸ©é˜µåˆ†é…
```python
# å‡è®¾ hid_dim=300, n_heads=6
# æ¯ä¸ªå¤´çš„ç»´åº¦: 300 Ã· 6 = 50
head_dim = hid_dim // n_heads  # 50

# æ¯ä¸ªå¤´ä½¿ç”¨å‚æ•°çŸ©é˜µçš„ä¸åŒéƒ¨åˆ†
head_1_params = W_Q[0:50, :]     # ç¬¬1ä¸ªå¤´ï¼šå‚æ•°çŸ©é˜µçš„å‰50è¡Œ
head_2_params = W_Q[50:100, :]   # ç¬¬2ä¸ªå¤´ï¼šå‚æ•°çŸ©é˜µçš„ç¬¬50-100è¡Œ
# ... ä»¥æ­¤ç±»æ¨
```

#### ç»´åº¦å˜åŒ–è¿‡ç¨‹
```python
# è¾“å…¥: [64, 12, 300] - 64ä¸ªå¥å­ï¼Œ12ä¸ªè¯ï¼Œæ¯è¯300ç»´

# 1. è®¡ç®—Q,K,V
Q = self.w_q(input)  # [64, 12, 300]

# 2. åˆ†å‰²ä¸ºå¤šå¤´
Q = Q.view(64, 12, 6, 50)      # åˆ†æˆ6ä¸ªå¤´ï¼Œæ¯å¤´50ç»´
Q = Q.permute(0, 2, 1, 3)      # [64, 6, 12, 50] - è½¬ç½®æ–¹ä¾¿è®¡ç®—

# 3. æ¯ä¸ªå¤´ç‹¬ç«‹è®¡ç®—æ³¨æ„åŠ›
attention = torch.matmul(Q, K.permute(0, 1, 3, 2)) / scale
# ç»“æœï¼š[64, 6, 12, 12] - 64ä¸ªå¥å­ï¼Œ6ä¸ªå¤´ï¼Œ12Ã—12ä¸ªattentionåˆ†æ•°

# 4. åº”ç”¨åˆ°Value
output = torch.matmul(attention, V)  # [64, 6, 12, 50]

# 5. æ‹¼æ¥æ‰€æœ‰å¤´çš„ç»“æœ  
output = output.permute(0, 2, 1, 3)  # [64, 12, 6, 50]
output = output.view(64, 12, 300)    # [64, 12, 300] - æ¢å¤åŸå§‹ç»´åº¦
```

### æ³¨æ„åŠ›å¤´èåˆå®ä¾‹

ä»¥"it"è¿™ä¸ªè¯ä¸ºä¾‹ï¼š
```python
# å„ä¸ªå¤´äº§ç”Ÿä¸åŒè¡¨ç¤º
head1_output = [0.8, 0.2, 0.9, ..., 0.3]  # 50ç»´ï¼ŒæŒ‡ä»£ä¿¡æ¯
head2_output = [0.1, 0.7, 0.4, ..., 0.8]  # 50ç»´ï¼Œå› æœä¿¡æ¯  
head3_output = [0.5, 0.3, 0.1, ..., 0.6]  # 50ç»´ï¼Œè¯­æ³•ä¿¡æ¯
head4_output = [0.2, 0.9, 0.7, ..., 0.1]  # 50ç»´ï¼Œä½ç½®ä¿¡æ¯
head5_output = [0.6, 0.1, 0.8, ..., 0.4]  # 50ç»´ï¼Œè¯­ä¹‰ä¿¡æ¯
head6_output = [0.3, 0.5, 0.2, ..., 0.9]  # 50ç»´ï¼Œå…¶ä»–æ¨¡å¼

# æ‹¼æ¥æ‰€æœ‰å¤´
concat_output = torch.cat([
    head1_output,  # [50ç»´] æŒ‡ä»£ç‰¹å¾
    head2_output,  # [50ç»´] å› æœç‰¹å¾
    head3_output,  # [50ç»´] è¯­æ³•ç‰¹å¾
    head4_output,  # [50ç»´] ä½ç½®ç‰¹å¾
    head5_output,  # [50ç»´] è¯­ä¹‰ç‰¹å¾
    head6_output   # [50ç»´] å…¶ä»–ç‰¹å¾
], dim=-1)  # ç»“æœï¼š[300ç»´]

# çº¿æ€§å˜æ¢æ•´åˆä¿¡æ¯
final_output = self.fc(concat_output)  # æœ€ç»ˆ300ç»´è¡¨ç¤º
```

### è®­ç»ƒè¿‡ç¨‹ä¸­çš„å¤´éƒ¨åˆ†åŒ–
- **è®­ç»ƒå‰**ï¼šæ‰€æœ‰æ³¨æ„åŠ›å¤´å‚æ•°éšæœºåˆå§‹åŒ–ï¼Œæ— æ³•é¢„çŸ¥å„å¤´ä¼šå­¦ä¹ ä»€ä¹ˆ
- **è®­ç»ƒå**ï¼šè‡ªç„¶æ¶Œç°ä¸“é—¨åŒ–ï¼Œä¸åŒå¤´è‡ªåŠ¨å­¦ä¼šä¸åŒç±»å‹çš„è¯­è¨€å…³ç³»
- **é‡è¦**ï¼šè¿™ç§ä¸“é—¨åŒ–æ˜¯**è‡ªç„¶æ¶Œç°**çš„ï¼Œä¸æ˜¯äººä¸ºè®¾è®¡çš„

---

## å¹¶è¡Œè®¡ç®—ä¸çŸ©é˜µè¿ç®—

### Transformer vs RNNçš„å¹¶è¡Œæ€§å¯¹æ¯”

#### RNNçš„ä¸²è¡Œå¤„ç†ï¼ˆæ— æ³•å¹¶è¡Œï¼‰
```python
# RNNå¿…é¡»æŒ‰æ—¶é—´æ­¥é€ä¸ªå¤„ç†
sentence = ["The", "animal", "didn't", "cross"]

# ä¸²è¡Œå¤„ç†ï¼š
h1 = RNN(word="The", prev_h=h0)           # ç¬¬1æ­¥
h2 = RNN(word="animal", prev_h=h1)        # ç¬¬2æ­¥ï¼Œä¾èµ–h1
h3 = RNN(word="didn't", prev_h=h2)        # ç¬¬3æ­¥ï¼Œä¾èµ–h2  
h4 = RNN(word="cross", prev_h=h3)         # ç¬¬4æ­¥ï¼Œä¾èµ–h3
```

#### Transformerçš„å¹¶è¡Œå¤„ç†
```python
# Transformerå¯ä»¥åŒæ—¶å¤„ç†æ‰€æœ‰è¯
sentence_matrix = [
    [word1_embedding],  # "The"
    [word2_embedding],  # "animal" 
    [word3_embedding],  # "didn't"
    [word4_embedding]   # "cross"
]  # å½¢çŠ¶ï¼š[4, 300]

# ä¸€æ¬¡çŸ©é˜µè¿ç®—å¤„ç†æ‰€æœ‰è¯
Q = sentence_matrix @ W_Q  # [4,300] @ [300,300] = [4,300]
K = sentence_matrix @ W_K  # åŒæ—¶è®¡ç®—æ‰€æœ‰è¯çš„K
V = sentence_matrix @ W_V  # åŒæ—¶è®¡ç®—æ‰€æœ‰è¯çš„V
```

### å¹¶è¡Œè®¡ç®—çš„å®é™…è§„æ¨¡
```python
# å®é™…çš„å¹¶è¡Œè®¡ç®—é‡
batch_size = 64      # 64ä¸ªå¥å­
seq_len = 12         # æ¯å¥12ä¸ªè¯
n_heads = 6          # 6ä¸ªæ³¨æ„åŠ›å¤´

# ä¸€æ¬¡çŸ©é˜µè¿ç®—åŒæ—¶è®¡ç®—ï¼š
total_attention_scores = 64 Ã— 6 Ã— 12 Ã— 12 = 55,296ä¸ªattentionåˆ†æ•°

# å…³é”®ï¼šè¿™55,296ä¸ªåˆ†æ•°æ˜¯å¹¶è¡Œè®¡ç®—çš„ï¼Œä¸æ˜¯å¾ªç¯è®¡ç®—
attention = torch.matmul(Q, K.permute(0, 1, 3, 2))  # ä¸€æ­¥å®Œæˆ
```

### æ¶æ„æ¾„æ¸…ï¼šä¸æ˜¯å¤šä¸ªæ¨¡å—ï¼Œè€Œæ˜¯çŸ©é˜µåˆ†å‰²
```python
# âŒ é”™è¯¯ç†è§£ï¼šåˆ›å»º6Ã—12=72ä¸ªç‹¬ç«‹æ³¨æ„åŠ›å¤´æ¨¡å—
for word in words:
    for head in heads:
        attention_module = create_new_module()  # é”™è¯¯ï¼

# âœ… æ­£ç¡®å®ç°ï¼š1ä¸ªæ¨¡å—é€šè¿‡çŸ©é˜µè¿ç®—å¤„ç†æ‰€æœ‰
class MultiheadAttention(nn.Module):
    def __init__(self, hid_dim=300, n_heads=6):
        # åªæœ‰1å¥—å‚æ•°çŸ©é˜µï¼Œé€šè¿‡viewå’Œpermuteåˆ†å‰²ç»™å¤šä¸ªå¤´
        self.w_q = nn.Linear(300, 300)  # 1ä¸ªW^QçŸ©é˜µ
        self.w_k = nn.Linear(300, 300)  # 1ä¸ªW^KçŸ©é˜µ
        self.w_v = nn.Linear(300, 300)  # 1ä¸ªW^VçŸ©é˜µ
```

### æ¯ä¸ªå•è¯çš„å¤„ç†æ–¹å¼
**æ˜¯çš„ï¼Œæ¯ä¸ªå•è¯éƒ½è¢«æ‰€æœ‰6ä¸ªæ³¨æ„åŠ›å¤´åŒæ—¶å¤„ç†**ï¼š

```python
# å¥å­ä¸­çš„æ‰€æœ‰å•è¯åŒæ—¶å¤„ç†
words = ["The", "animal", "didn't", "cross", "street", "because", "it", "was", "too", "tired"]

# æ¯ä¸ªè¯éƒ½ä¼šè¢«6ä¸ªæ³¨æ„åŠ›å¤´åŒæ—¶å¤„ç†
for word in words:
    # è¿™6ä¸ªå¤´çš„è®¡ç®—æ˜¯å¹¶è¡Œçš„ï¼Œä¸æ˜¯ä¸²è¡Œçš„
    head1_output = attention_head1(word)  # è¯­æ³•å…³ç³»
    head2_output = attention_head2(word)  # è¯­ä¹‰å…³ç³»  
    head3_output = attention_head3(word)  # ä½ç½®å…³ç³»
    head4_output = attention_head4(word)  # æŒ‡ä»£å…³ç³»
    head5_output = attention_head5(word)  # ä¾å­˜å…³ç³»
    head6_output = attention_head6(word)  # å…¶ä»–æ¨¡å¼
    
    # èåˆæ‰€æœ‰å¤´çš„ç»“æœ
    final_word_representation = concat([head1_output, ..., head6_output])
```

---

## QKVæœºåˆ¶è¯¦è§£

### QKVçš„ç›´è§‚ç†è§£
æŠŠæ³¨æ„åŠ›æœºåˆ¶æƒ³è±¡æˆ**å›¾ä¹¦é¦†æ£€ç´¢ç³»ç»Ÿ**ï¼š
- **Queryï¼ˆæŸ¥è¯¢ï¼‰**ï¼š"æˆ‘æƒ³æ‰¾å…³äº'æœºå™¨å­¦ä¹ 'çš„ä¹¦"
- **Keyï¼ˆç´¢å¼•ï¼‰**ï¼šæ¯æœ¬ä¹¦çš„å…³é”®è¯æ ‡ç­¾  
- **Valueï¼ˆå†…å®¹ï¼‰**ï¼šæ¯æœ¬ä¹¦çš„å®é™…å†…å®¹

### ä¸åŒå±‚ä¸­QKVçš„æ¥æºå’Œå«ä¹‰

#### ç¼–ç å™¨Self-Attention
```python
# Qã€Kã€Véƒ½æ¥è‡ªåŒä¸€ä¸ªè¾“å…¥åºåˆ—
encoder_input = ["The", "animal", "didn't", "cross"]

Q = encoder_input @ W_Q  # Queryï¼šæˆ‘è¦å…³æ³¨ä»€ä¹ˆï¼Ÿ
K = encoder_input @ W_K  # Keyï¼šæˆ‘èƒ½è¢«ä»€ä¹ˆå…³æ³¨ï¼Ÿ  
V = encoder_input @ W_V  # Valueï¼šæˆ‘åŒ…å«ä»€ä¹ˆä¿¡æ¯ï¼Ÿ
```

**å…·ä½“å«ä¹‰**ï¼š
- **Q_it**ï¼šè¡¨ç¤º"itæƒ³è¦å…³æ³¨ä»€ä¹ˆ"
- **K_animal, K_tired...**ï¼šè¡¨ç¤ºå…¶ä»–è¯"èƒ½è¢«å…³æ³¨çš„ç‰¹å¾"
- **V_animal, V_tired...**ï¼šè¡¨ç¤ºå…¶ä»–è¯"å®é™…åŒ…å«çš„è¯­ä¹‰ä¿¡æ¯"

#### è§£ç å™¨Self-Attentionï¼ˆæ©ç è‡ªæ³¨æ„åŠ›ï¼‰
```python
# Qã€Kã€Véƒ½æ¥è‡ªè§£ç å™¨è‡ªå·±çš„è¾“å…¥ï¼Œä½†åº”ç”¨mask
decoder_input = ["<start>", "I", "am"]

Q = decoder_input @ W_Q
K = decoder_input @ W_K  
V = decoder_input @ W_V

# åº”ç”¨Future Maskï¼Œé˜²æ­¢çœ‹åˆ°æœªæ¥çš„è¯
if mask is not None:
    attention = attention.masked_fill(mask == 0, -1e10)
```

#### è§£ç å™¨Encoder-Decoder Attentionï¼ˆæœ€å…³é”®ï¼‰
```python
# æœºå™¨ç¿»è¯‘ä¾‹å­ï¼šæ³•è¯­ â†’ è‹±è¯­
encoder_output = encode("Je suis Ã©tudiant")  # ç¼–ç å™¨è¾“å‡º

# è§£ç å™¨ç”Ÿæˆ"I"æ—¶
decoder_state = decode_step("I")

# Encoder-Decoder Attentionï¼š
Q = decoder_state @ W_Q      # "å½“å‰è¦ç”Ÿæˆçš„è‹±è¯­è¯æƒ³å…³æ³¨ä»€ä¹ˆæ³•è¯­è¯ï¼Ÿ"
K = encoder_output @ W_K     # "æ³•è¯­å¥å­ä¸­æ¯ä¸ªè¯çš„ç‰¹å¾"
V = encoder_output @ W_V     # "æ³•è¯­å¥å­ä¸­æ¯ä¸ªè¯çš„è¯­ä¹‰å†…å®¹"
```

**å…³é”®åŒºåˆ«**ï¼š
- **Qæ¥æº**ï¼šè§£ç å™¨å½“å‰çŠ¶æ€
- **Kã€Væ¥æº**ï¼šç¼–ç å™¨çš„è¾“å‡º  
- **ä½œç”¨**ï¼šå®ç°è·¨è¯­è¨€å¯¹é½å’Œä¿¡æ¯ä¼ é€’

---

## Dropoutæœºåˆ¶æ·±å…¥ç†è§£

### Dropoutçš„ä½œç”¨æœºåˆ¶
```python
# è®­ç»ƒæ—¶ï¼ˆ.train()æ¨¡å¼ï¼‰
self.do = nn.Dropout(0.1)  # 10%çš„è¿æ¥è¢«éšæœºä¸¢å¼ƒ
attention = self.do(torch.softmax(attention, dim=-1))

# æ¨ç†æ—¶ï¼ˆ.eval()æ¨¡å¼ï¼‰  
# Dropoutè‡ªåŠ¨å…³é—­ï¼Œæ‰€æœ‰è¿æ¥ä¿ç•™
model.eval()
with torch.no_grad():
    output = model(input)  # è¾“å‡ºç¨³å®šä¸€è‡´
```

### ä¸ºä»€ä¹ˆè¦"ä¸¢å¼ƒ"ç¥ç»å…ƒï¼Ÿ
1. **é˜²æ­¢è¿‡æ‹Ÿåˆ**ï¼šé¿å…æ¨¡å‹è¿‡åº¦ä¾èµ–ç‰¹å®šç¥ç»å…ƒç»„åˆ
2. **æé«˜æ³›åŒ–èƒ½åŠ›**ï¼šè¿«ä½¿æ¨¡å‹å­¦ä¹ æ›´é²æ£’çš„ç‰¹å¾è¡¨ç¤º
3. **æ¨¡æ‹Ÿé›†æˆå­¦ä¹ **ï¼šæ¯æ¬¡è®­ç»ƒéƒ½æ˜¯ç•¥æœ‰ä¸åŒçš„å­ç½‘ç»œåœ¨å­¦ä¹ 

### åœ¨Transformeræ³¨æ„åŠ›ä¸­çš„å…·ä½“ä½œç”¨
- **é˜²æ­¢è¿‡åº¦å…³æ³¨**ï¼šé¿å…æ€»æ˜¯è¿‡åº¦ä¾èµ–æŸäº›ç‰¹å®šè¯æ±‡å…³è”
- **å¢å¼ºé²æ£’æ€§**ï¼šè®©æ¨¡å‹å­¦ä¼šåˆ©ç”¨å¤šæ ·åŒ–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
- **å‡å°‘å…±é€‚åº”**ï¼šä¸åŒattention headä¹‹é—´ä¸ä¼šè¿‡åº¦ä¾èµ–

### é‡è¦æ¦‚å¿µæ¾„æ¸…
**Dropoutä¸­è¢«"ä¸¢å¼ƒ"çš„ç¥ç»å…ƒä¸èƒ½ç›´æ¥åˆ é™¤**ï¼š
- **éšæœºæ€§æ˜¯æ ¸å¿ƒ**ï¼šæ¯æ¬¡ä¸¢å¼ƒçš„ç¥ç»å…ƒä¸åŒ
- **æ¨ç†éœ€è¦å®Œæ•´ç½‘ç»œ**ï¼šæ‰€æœ‰ç¥ç»å…ƒåœ¨æ¨ç†æ—¶éƒ½æœ‰è´¡çŒ®
- **ä¸ç¥ç»ç½‘ç»œå‰ªæä¸åŒ**ï¼šdropoutæ˜¯æ­£åˆ™åŒ–ï¼Œå‰ªææ˜¯å‹ç¼©

---

## å®é™…åº”ç”¨ä¸ä»£ç å®ç°

### å®Œæ•´çš„MultiheadAttentionå®ç°
```python
class MultiheadAttention(nn.Module):
    def __init__(self, hid_dim, n_heads, dropout):
        super(MultiheadAttention, self).__init__()
        self.hid_dim = hid_dim
        self.n_heads = n_heads
        assert hid_dim % n_heads == 0
        self.w_q = nn.Linear(hid_dim, hid_dim)
        self.w_k = nn.Linear(hid_dim, hid_dim)
        self.w_v = nn.Linear(hid_dim, hid_dim)
        self.fc = nn.Linear(hid_dim, hid_dim)
        self.do = nn.Dropout(dropout)
        self.scale = torch.sqrt(torch.FloatTensor([hid_dim // n_heads]))
    def forward(self, query, key, value, mask=None):
        bsz = query.shape[0]
        Q = self.w_q(query)
        K = self.w_k(key)
        V = self.w_v(value)
        Q = Q.view(bsz, -1, self.n_heads, self.hid_dim // self.n_heads).permute(0, 2, 1, 3)
        K = K.view(bsz, -1, self.n_heads, self.hid_dim // self.n_heads).permute(0, 2, 1, 3)
        V = V.view(bsz, -1, self.n_heads, self.hid_dim // self.n_heads).permute(0, 2, 1, 3)
        attention = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale
        if mask is not None:
            attention = attention.masked_fill(mask == 0, -1e10)
        attention = self.do(torch.softmax(attention, dim=-1))
        x = torch.matmul(attention, V)
        x = x.permute(0, 2, 1, 3).contiguous()
        x = x.view(bsz, -1, self.n_heads * (self.hid_dim // self.n_heads))
        x = self.fc(x)
        return x
```

### ä½¿ç”¨ç¤ºä¾‹
```python
attention = MultiheadAttention(hid_dim=300, n_heads=6, dropout=0.1)
query = torch.rand(64, 12, 300)
key = torch.rand(64, 10, 300)
value = torch.rand(64, 10, 300)
output = attention(query, key, value)  # è¾“å‡ºï¼š[64, 12, 300]
```

### è®¡ç®—æœºå¦‚ä½•åˆ©ç”¨attentionè¾“å‡º

#### 1. åœ¨ç¼–ç å™¨ä¸­çš„æµå‘
```python
class EncoderLayer(nn.Module):
    def forward(self, x):
        attention_output = self.multi_head_attention(x, x, x)
        x = self.layer_norm1(x + attention_output)
        ffn_output = self.feed_forward(x)
        x = self.layer_norm2(x + ffn_output)
        return x
```

#### 2. åœ¨ä¸åŒä»»åŠ¡ä¸­çš„åº”ç”¨
```python
# æ–‡æœ¬åˆ†ç±»ä»»åŠ¡
cls_representation = encoder_output[:, 0, :]
class_logits = classifier(cls_representation)

# åºåˆ—æ ‡æ³¨ä»»åŠ¡  
tag_logits = tag_classifier(encoder_output)

# æœºå™¨ç¿»è¯‘ä»»åŠ¡ï¼ˆç¼–è§£ç å™¨æ³¨æ„åŠ›ï¼‰
decoder_output = decoder(
    target_sequence,
    encoder_output,  # ä½œä¸ºKã€Vè¾“å…¥
    encoder_output
)
```

### ç»´åº¦å˜åŒ–å…¨è¿‡ç¨‹è¿½è¸ª
```python
input_shape = [64, 12, 300]
Q_shape = [64, 12, 300]
K_shape = [64, 12, 300]
V_shape = [64, 12, 300]
Q_reshaped = [64, 12, 6, 50]
Q_permuted = [64, 6, 12, 50]
attention_scores = [64, 6, 12, 12]
attention_output = [64, 6, 12, 50]
concat_output = [64, 12, 300]
final_output = [64, 12, 300]
```

---

## æ€»ç»“ä¸å…³é”®è¦ç‚¹

### æ ¸å¿ƒæ¦‚å¿µå›é¡¾
1. **Transformerçš„æœ¬è´¨**ï¼šåŸºäºSelf-Attentionçš„å¹¶è¡Œåºåˆ—å¤„ç†æ¶æ„
2. **å¤šå¤´æ³¨æ„åŠ›**ï¼šé€šè¿‡å¤šä¸ªå­ç©ºé—´å­¦ä¹ ä¸åŒç±»å‹çš„è¯­è¨€å…³ç³»
3. **å¹¶è¡Œè®¡ç®—**ï¼šé€šè¿‡çŸ©é˜µè¿ç®—å®ç°é«˜æ•ˆå¹¶è¡Œï¼Œé¿å…RNNçš„ä¸²è¡Œä¾èµ–
4. **QKVæœºåˆ¶**ï¼šQuery-Key-Valueçš„æ³¨æ„åŠ›è®¡ç®—èŒƒå¼
5. **ä½ç½®ç¼–ç **ï¼šä¸ºæ¨¡å‹æä¾›åºåˆ—ä½ç½®ä¿¡æ¯

### é‡è¦æŠ€æœ¯ç»†èŠ‚
- **å‚æ•°åˆå§‹åŒ–**ï¼šW^Qã€W^Kã€W^Véšæœºåˆå§‹åŒ–ï¼Œè®­ç»ƒåè‡ªç„¶åˆ†åŒ–
- **Maskæœºåˆ¶**ï¼šå±è”½paddingå’Œæœªæ¥ä½ç½®ï¼Œç¡®ä¿æ³¨æ„åŠ›çš„æ­£ç¡®æ€§
- **æ®‹å·®è¿æ¥**ï¼šå¸®åŠ©æ¢¯åº¦ä¼ æ’­ï¼Œç¨³å®šè®­ç»ƒè¿‡ç¨‹
- **Dropoutæ­£åˆ™åŒ–**ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæé«˜æ³›åŒ–èƒ½åŠ›

### å®è·µè¦ç‚¹
- **ç¯å¢ƒé…ç½®**ï¼šè™šæ‹Ÿç¯å¢ƒéš”ç¦»ï¼Œç‰ˆæœ¬å…¼å®¹æ€§ç®¡ç†
- **ä»£ç å®ç°**ï¼šç†è§£çŸ©é˜µæ“ä½œèƒŒåçš„æ•°å­¦åŸç†
- **è°ƒè¯•æŠ€å·§**ï¼šé€šè¿‡ç»´åº¦è¿½è¸ªç†è§£æ•°æ®æµå‘
- **æ€§èƒ½ä¼˜åŒ–**ï¼šåˆ©ç”¨GPUå¹¶è¡Œè®¡ç®—èƒ½åŠ›

è¿™ä»½å®Œæ•´çš„å­¦ä¹ è®°å½•æ¶µç›–äº†ä»ç¯å¢ƒæ­å»ºåˆ°Transformeræ·±å±‚åŸç†çš„å…¨éƒ¨å†…å®¹ï¼Œä¸ºæ·±å…¥ç†è§£å’Œåº”ç”¨Transformeræ¨¡å‹æä¾›äº†åšå®çš„ç†è®ºåŸºç¡€å’Œå®è·µæŒ‡å¯¼ã€‚
